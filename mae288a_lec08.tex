\mainmatter%
\setcounter{page}{1}

\lectureseries[\course]{\course}

\auth[\lecAuth]{Lecturer: \lecAuth\\ Scribe: \scribe}
\date{October 22, 2009}

\setaddress%

% the following hack starts the lecture numbering at 8
\setcounter{lecture}{7}
\setcounter{chapter}{7}

\lecture{Value Iteration Convergence}

\section{Contraction Mapping Theorem Review}
The Contraction Mapping Principle, or Banach Fixed Point Theorem, was given in Theorem~\ref{th:bfp}.
We can use this to show that iterative value functions will converge to a solution.

\section{Dynamic Programming Equation}
The DPE has
\begin{align*}
V&=\mathcal{G}[V] \\
J(x,w) &= E[\sum\alpha^t l(\xi_t w_t(\xi_t))] \\
\xi_{t+1} &= f(\xi_t,u+t,w_t) \\
V(x) &= \inf_{\mu\in\mathcal{M}}J(x,\mu)
\end{align*}

We want to apply Theorem~\ref{th:bfp} to bounded functions.

\subsection{Bounded Functions}
Let

\begin{equation*}
\mathbb{Y} = C_B(\mathbb{R}^N) = \{V:\mathbb{R}^n\to\mathbb{R} | V \text{~cont.,~} \exists C_v \text{~s.t.~} |V(x)|\leq C_v~\forall x\in\mathbb{R}^n\}
\end{equation*}

Note that $C_v$ depends on $V$.
Let

\begin{equation*}
||V||_\infty = \sup_{x\in\mathbb{R}^n}|V(x)|
\end{equation*}

and $\vsp$ be a Banach space.

Assume that there exists $M<\infty$ such that $|l(x,u)|\leq M~\forall x,u$.
Then a bounded function is
\begin{align*}
\epsilon&>0 \exists \delta>0 \text{~s.t.~} |l(x,u)-l(z,u)|<\epsilon~\forall x,z\in\mathbb{R}^n \text{~s.t.~} |x-z|<\delta \\
\epsilon&>0 \exists \delta>0 \text{~s.t.~} |F(x,u,w)-F(z,u,w)|<\epsilon \\
&\qquad\qquad~\forall u\in\mathcal{U},w\in\mathbb{R}^n,~\forall x,z \text{~s.t.~} |x-z|<\delta
\end{align*}

\subsection{Show $\mathcal{G}$ Maps Into $\mathbb{R}^n$}
For iterative value functions to converge to a solution we need
\begin{enumerate}
\item $\mathcal{G}$ to map into the space.
\item $F$ is a contraction.
\end{enumerate}
Suppose $||V||_\infty<C_v$.
When we apply $\mathcal{G}$ we want $|\mathcal{G}[V](x)|$ to be bounded.
\begin{align*}
|V(f(x,u,w))| &\leq C_v \\
\Rightarrow E[|V(f(x,u,w))|] &= E[\int|V(f(x,u,w))|p_w(w)dw] \leq C_v \\
\Rightarrow |\alpha E[V(f(x,u,w))]| &\leq \alpha C_v
\end{align*}
but
\begin{align*}
|l(x,u)| &\leq M~\forall x,u \\
\therefore |\mathcal{G}[V](x)| &\leq M + \alpha C_v \\
\Rightarrow ||\mathcal{G}[V]||_\infty &\leq M + \alpha C_v
\end{align*}
We claim without proving that $\mathcal{G}[V](\cdot)$ is continuous.
This leads to $\mathcal{G}:\mathbb{Y}\to\mathbb{Y}$ which shows that $\mathcal{G}$ maps into the space.
We still need to show that $F$ is a contraction.

\begin{theorem}%
\label{th:nvs}
Let $F,H:\mathbb{X}\to\mathbb{R}$, then $\mathbb{X}$ is a normed vector space.
\begin{align*}
|\sup_{x\in\mathbb{X}}F(x) - \sup_{z\in\mathbb{X}}H(z)| &\leq \sup_{x\in\mathbb{X}}|F(x)-H(x)| \\
|\inf_{x\in\mathbb{X}}F(x) - \inf_{z\in\mathbb{X}}H(z)| &\leq \sup_{x\in\mathbb{X}}|F(x)-H(x)|
\end{align*}
\end{theorem}

\subsection{Show that $F$ is a contraction.}%
\label{sec:showfcontraction}
Let $V,\tilde{V}\in\mathbb{Y}$, we need to show that $||\mathcal{G}[V]-\mathcal{G}[\tilde{V}]||_\infty \leq ||V-\tilde{V}||_\infty$.
\begin{align*}
&|V(f(x,u,w)) - \tilde{V}(f(x,u,w))| \leq ||V-\tilde{V}||_\infty \\
\Rightarrow &E[|V(f(x,u,w)) - \tilde{V}(f(x,u,w))|] \leq ||V-\tilde{V}||_\infty \\
\Rightarrow &|l(x,u)+\alpha E[V(f(x,u,w))] - l(x,u)+\alpha E[\tilde{V}(f(x,u,w))]| \\
&\qquad \leq \alpha||V-\tilde{V}||_\infty~\forall x,u
\end{align*}
Using Theorem~\ref{th:nvs} we have
\begin{align*}
&|\inf_u\{l(x,u)+\alpha E[V(f(x,u,w))]\} - \inf_u\{l(x,u)+\alpha E[\tilde{V}(f(x,u,w))]\}| \\
&\qquad \leq ||V-\tilde{V}||_\infty \\
\Rightarrow &||\mathcal{G}[V]-\mathcal{G}[\tilde{V}]||_\infty \leq \alpha||V-\tilde{V}||_\infty
\end{align*}
Under the conditions that $\mathcal{G}$ is continuous and bounded and that $F$ is a contraction then the value iteration converges to the solution.

\begin{example}
Given
\begin{align*}
\xi_{t+1} &= \xi_t+u_t+w_t \\
u_t\in\mathcal{U} &= \{0,-1\} \\
\{w_t\} &\text{~is~IID} \\
p_{w_t}(w) &= \begin{cases} 1, & w\in[0,1) \\ 0, & w\notin[0,1) \end{cases} \\% chktex 9
l(x,u) &= \begin{cases} e^{\frac{x}{2}}, & u=0 \\ e^x, & u=-1 \end{cases} \\
J(x,w) &= E\left[\sum_{t=0}^\infty \alpha^t l(\xi_t,w_t(\xi_t))\right] \\
V(x) &= \min\{e^{\frac{x}{2}} + \alpha E[V(x+w)], e^x + \alpha E[V(x-1+w)]\}
\end{align*}
We start by guessing a solution $V_0(x)=0~\forall x$.
Then
\begin{align*}
V_1(x) &= \mathcal{G}[V_0](x) = \min\{e^{\frac{x}{2}}+\alpha E[0], e^x + \alpha E[0]\} \\
&= \begin{cases} e^{\frac{x}{2}}, & x\geq0 \\ e^x, & x<0 \end{cases}
\end{align*}
If we stopped here the optimal control would be

\begin{equation*}
\mu(x) = \begin{cases} 0, & x\geq0 \\ -1, & x<0 \end{cases}
\end{equation*}

Continuing we have
\begin{align*}
V_2(x) &= \min\left\lbrace e^{\frac{x}{2}} + \alpha\left[\int_{-\infty}^{-x} e^{x+w}p_w(w)dw + \int_{-x}^\infty e^{\frac{x+w}{2}}p_w(w)dw\right], \right. \\
&\left.\qquad e^x+\alpha\left[\int_{-\infty}^{-(x-1)}e^{x-1+w}p_w(w)dw + \int_{-(x-1)}^\infty e^{x-1+w}p_w(w)dw\right]\right\rbrace
\end{align*}
This will generate a new optimal control, $\mu(x)$.
If we were to continue in this fashion until $||V_{n+1}-V_n||_\infty<\epsilon$, where $\epsilon$ is the convergence criterion selected for a problem, then we would get value iteration convergence.
Typically, the control, $\mu(x)$, converges to a steady-state faster than the value iteration so in practice this can be used instead of $||V_{n+1}-V_n||_\infty$.
$\lozenge$
\end{example}

\begin{example}
We will now look at the Linear Quadratic Gaussian (LQG) case.
Let
\begin{align*}
\xi_{t+1} &= A\xi_t+Bu_t+\sigma w_t \\
J(x,w) &= E\left[\sum_{t=0}^\infty \alpha^t\left[\frac{1}{2}\xi_t^T C\xi_t + \frac{1}{2}{(w_t \xi_t)}^T Dw_t \xi_t\right]\right] \\
V(x) &= \inf_{\mu\in\mathcal{M}}J(x,w) \\
\{u(t)\} &\in\mathbb{R}^l \\
\{w_t\} & \text{~IID,~} \sim N(0,Q)
\end{align*}
The DPE is

\begin{equation*}
V(x) = \inf_{u\in\mathbb{R}^l}\left\lbrace\tfrac{1}{2}x^T Cx + \tfrac{1}{2}u^T Du + \alpha E[V(Ax+Bu+\sigma w)]\}\right\rbrace
\end{equation*}

We start by guessing $V_0(x)=0 = \frac{1}{2}x^T P_0 x+R_0$, where $P_0=0$ is an $n\times n$ matrix and $R_0=0$.
Suppose that $V_k(x) = \frac{1}{2}x^T P_k x+R_k$ where $k$ is an iteration step and not a time step so that we can go from $V_0\to V_1$.
Then we get
\begin{align*}
V_{k+1}(x) &= \inf_u\left\lbrace\frac{1}{2}x^T Cx+\frac{1}{2}u^T Du + \frac{\alpha}{2}{(Ax+Bu)}^T P_k(Ax+Bu) \right.\\
&\left.\qquad + \frac{\alpha}{2}{(Ax+Bu)}^T P_k\sigma E[w] + \frac{\alpha}{2}E[{(\sigma w)}^T P_k \sigma w] + \alpha R_k\right\rbrace \\
&= \frac{1}{2}x^T Cx + R_{k+1} + \min_u\left\lbrace\underbrace{\frac{1}{2}u^T Du + \frac{\alpha}{2}{(Ax+Bu)}^T P_k (Ax+Bu)}_{G(u)}\right\rbrace
\end{align*}
We then want to minimize $G(u)$ such that
\begin{align*}
\nabla G(u) &= Du+\alpha B^T P_k(Ax+Bu) = Du + \alpha B^T P_k Ax + \alpha B^T P_k Bu = 0 \\
\Rightarrow u &= -\alpha{(D+\alpha B^T P_k B)}^{-1}B^T P_k x \doteq -\alpha K_k
\end{align*}
This leads to an expression for the value iteration as

\begin{equation*}
V_{k+1}(x) = \frac{1}{2}x^T P_{k+1}x + R_{k+1}
\end{equation*}

with

\begin{equation*}
P_{k+1} = \frac{\alpha^2}{2}K_k^T DK_k + \frac{\alpha}{2}{(A-\alpha BK_k)}^T P_k(A-\alpha BK_k)
\end{equation*}

$\lozenge$
\end{example}% chktex 17
